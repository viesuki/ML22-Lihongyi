{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want to access the version you have already modified, click \"Edit\"\n",
    "# If you want to access the original sample code, click \"...\", then click \"Copy & Edit Notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T09:55:39.859970Z",
     "iopub.status.busy": "2025-09-26T09:55:39.859752Z"
    },
    "papermill": {
     "duration": 19.351342,
     "end_time": "2022-02-23T10:03:06.247288",
     "exception": false,
     "start_time": "2022-02-23T10:02:46.895946",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./food11'):\n",
    "    for filename in filenames:\n",
    "        pass\n",
    "        #print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.0189,
     "end_time": "2022-02-23T10:03:06.279758",
     "exception": false,
     "start_time": "2022-02-23T10:03:06.260858",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_exp_name = \"strong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 1.654263,
     "end_time": "2022-02-23T10:03:07.947242",
     "exception": false,
     "start_time": "2022-02-23T10:03:06.292979",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary packages.\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image\n",
    "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "\n",
    "# This is for the progress bar.\n",
    "from tqdm.auto import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.078771,
     "end_time": "2022-02-23T10:03:08.039428",
     "exception": false,
     "start_time": "2022-02-23T10:03:07.960657",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "myseed = 6666  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01289,
     "end_time": "2022-02-23T10:03:08.065357",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.052467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Transforms**\n",
    "Torchvision provides lots of useful utilities for image preprocessing, data wrapping as well as data augmentation.\n",
    "\n",
    "Please refer to PyTorch official website for details about different transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.021406,
     "end_time": "2022-02-23T10:03:08.099437",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.078031",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Normally, We don't need augmentations in testing and validation.\n",
    "# All we need here is to resize the PIL image and transform it into Tensor.\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# However, it is also possible to use augmentation in the testing phase.\n",
    "# You may use train_tfm to produce a variety of images and then test using ensemble methods\n",
    "train_tfm = transforms.Compose([\n",
    "    # Resize the image into a fixed shape (height = width = 128)\n",
    "    transforms.RandomResizedCrop((128, 128), scale=(0.8, 1.0), ratio=(1.0, 1.0)),\n",
    "    # You may add some transforms here.\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.TrivialAugmentWide(),\n",
    "        transforms.RandAugment(),\n",
    "    ]),\n",
    "    # transforms.ColorJitter(brightness=0.4, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    # ToTensor() should be the last one of the transforms.\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    normalize,\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixup or Cutmix\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "num_classes, alpha = 11, 1.0\n",
    "mixup = transforms.MixUp(num_classes=num_classes, alpha=alpha)\n",
    "cutmix = transforms.CutMix(num_classes=num_classes, alpha=alpha)\n",
    "MorC = transforms.RandomChoice([mixup, cutmix], p=(0.5, 0.5))\n",
    "collate_fn = lambda batch: MorC(*default_collate(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012739,
     "end_time": "2022-02-23T10:03:08.125181",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.112442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Datasets**\n",
    "The data is labelled by the name, so we load images and label while calling '__getitem__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.023022,
     "end_time": "2022-02-23T10:03:08.160912",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.13789",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "\n",
    "    def __init__(self,path=None,tfm=test_tfm,files = None, is_test=False):\n",
    "        super(FoodDataset).__init__()\n",
    "        self.path = path\n",
    "        self.is_test = is_test\n",
    "        # self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "        self.files = sorted(files) if files != None else sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "        if files != None:\n",
    "            self.files = files\n",
    "        print(f\"One {path} sample\",self.files[0])\n",
    "        self.transform = tfm\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "        im = self.transform(im)\n",
    "        # im = self.data[idx]\n",
    "        if self.is_test:\n",
    "            return im\n",
    "        else:\n",
    "            label = int(fname.split(\"\\\\\")[-1].split(\"_\")[0]) # test has no label\n",
    "            return im, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.0258,
     "end_time": "2022-02-23T10:03:08.199437",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.173637",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyResidual(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MyResidual, self).__init__()\n",
    "\n",
    "            self.cnn_layer1 = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, 3, 1, 1),\n",
    "                nn.BatchNorm2d(64),         # [64, 128, 128]\n",
    "            )\n",
    "\n",
    "            self.cnn_layer2 = nn.Sequential(\n",
    "                nn.Conv2d(64, 64, 3, 1, 1),\n",
    "                nn.BatchNorm2d(64),         # [64, 128, 128]\n",
    "            )\n",
    "\n",
    "            self.cnn_layer3 = nn.Sequential(\n",
    "                nn.Conv2d(64, 128, 3, 2, 1),\n",
    "                nn.BatchNorm2d(128),        # [128, 64, 64]\n",
    "            )\n",
    "\n",
    "            self.cnn_layer4 = nn.Sequential(\n",
    "                nn.Conv2d(128, 128, 3, 1, 1),\n",
    "                nn.BatchNorm2d(128),        # [128, 64, 64]\n",
    "            )\n",
    "            self.cnn_layer5 = nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 3, 2, 1),\n",
    "                nn.BatchNorm2d(256),        # [256, 32, 32]\n",
    "            )\n",
    "            self.cnn_layer6 = nn.Sequential(\n",
    "                nn.Conv2d(256, 256, 3, 1, 1),\n",
    "                nn.BatchNorm2d(256),        # [256, 32, 32]\n",
    "            )\n",
    "            self.cnn_layer7 = nn.Sequential(\n",
    "                nn.Conv2d(256, 512, 3, 2, 1),\n",
    "                nn.BatchNorm2d(512),        # [512, 16, 16]\n",
    "            )\n",
    "            \n",
    "            self.fc_layer = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d((4, 4)), # [512, 4, 4]\n",
    "                nn.Flatten(),               # [512 * 4 * 4]\n",
    "                nn.Linear(512 * 4 * 4, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                \n",
    "                nn.Linear(512, 11)\n",
    "            )\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "        def forward(self, x):\n",
    "            # input (x): [batch_size, 3, 128, 128]\n",
    "            # output: [batch_size, 11]\n",
    "\n",
    "            # Extract features by convolutional layers.\n",
    "            x1 = self.cnn_layer1(x)\n",
    "\n",
    "            x1 = self.relu(x1)\n",
    "\n",
    "            x2 = self.cnn_layer2(x1) + x1\n",
    "\n",
    "            x2 = self.relu(x2)\n",
    "\n",
    "            x3 = self.cnn_layer3(x2)\n",
    "\n",
    "            x3 = self.relu(x3) + x3\n",
    "\n",
    "            x4 = self.cnn_layer4(x3)\n",
    "\n",
    "            x4 = self.relu(x4)\n",
    "\n",
    "            x5 = self.cnn_layer5(x4)\n",
    "\n",
    "            x5 = self.relu(x5)\n",
    "\n",
    "            x6 = self.cnn_layer6(x5) + x5\n",
    "\n",
    "            x6 = self.relu(x6)\n",
    "\n",
    "            # The extracted feature map must be flatten before going to fully-connected layers.\n",
    "            xout = self.cnn_layer7(x6)\n",
    "\n",
    "            # The features are transformed by fully-connected layers to obtain the final logits.\n",
    "            xout = self.fc_layer(xout)\n",
    "            return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet34, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet34(weights=None)\n",
    "        self.fc_feat = self.resnet.fc.in_features\n",
    "        \n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.fc_feat, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 11)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.054295,
     "end_time": "2022-02-23T10:03:08.266338",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.212043",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# _dataset_dir = \"./food11\"\n",
    "# # Construct datasets.\n",
    "# # The argument \"loader\" tells how torchvision reads the data.\n",
    "# train_set = FoodDataset(os.path.join(_dataset_dir,\"training\"), tfm=train_tfm)\n",
    "# train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "# valid_set = FoodDataset(os.path.join(_dataset_dir,\"validation\"), tfm=test_tfm)\n",
    "# valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One None sample ./food11\\training\\2_986.jpg\n",
      "One None sample ./food11\\training\\2_578.jpg\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "_dataset_dir = \"./food11\"\n",
    "\n",
    "# mix train and validation data and re-divide as 8:2\n",
    "trpath, vapath = os.path.join(_dataset_dir,\"training\"), os.path.join(_dataset_dir,\"validation\")\n",
    "all_files = [os.path.join(trpath,x) for x in os.listdir(trpath) if x.endswith(\".jpg\")\n",
    "             ] + [os.path.join(vapath,x) for x in os.listdir(vapath) if x.endswith(\".jpg\")]\n",
    "\n",
    "# randomly shuffle and split\n",
    "np.random.shuffle(all_files)\n",
    "train_files, valid_files = all_files[:int(len(all_files)*0.8)], all_files[int(len(all_files)*0.8):]\n",
    "\n",
    "# load data\n",
    "train_set = FoodDataset(tfm=train_tfm, files=train_files)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=0, pin_memory=True,\n",
    "                          drop_last=True, collate_fn=collate_fn)\n",
    "valid_set = FoodDataset(tfm=test_tfm, files=valid_files)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128]) torch.Size([32, 11])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 32830.720158,
     "end_time": "2022-02-23T19:10:19.001001",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.280843",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 001/050 ] loss = 1.37985, acc = 0.77334\n",
      "[ Valid | 001/050 ] loss = 0.98800, acc = 0.83594\n",
      "[ Valid | 001/050 ] loss = 0.98800, acc = 0.83594 -> best\n",
      "Best model found at epoch 0, saving model\n",
      "[ Train | 002/050 ] loss = 1.40429, acc = 0.76694\n",
      "[ Valid | 002/050 ] loss = 0.93690, acc = 0.83891\n",
      "[ Valid | 002/050 ] loss = 0.93690, acc = 0.83891 -> best\n",
      "Best model found at epoch 1, saving model\n",
      "[ Train | 003/050 ] loss = 1.42872, acc = 0.75066\n",
      "[ Valid | 003/050 ] loss = 0.96920, acc = 0.83780\n",
      "[ Valid | 003/050 ] loss = 0.96920, acc = 0.83780\n",
      "[ Train | 004/050 ] loss = 1.39977, acc = 0.77137\n",
      "[ Valid | 004/050 ] loss = 0.95327, acc = 0.83743\n",
      "[ Valid | 004/050 ] loss = 0.95327, acc = 0.83743\n",
      "[ Train | 005/050 ] loss = 1.41527, acc = 0.75104\n",
      "[ Valid | 005/050 ] loss = 0.97004, acc = 0.83482\n",
      "[ Valid | 005/050 ] loss = 0.97004, acc = 0.83482\n",
      "[ Train | 006/050 ] loss = 1.39248, acc = 0.76064\n",
      "[ Valid | 006/050 ] loss = 0.93882, acc = 0.84115\n",
      "[ Valid | 006/050 ] loss = 0.93882, acc = 0.84115 -> best\n",
      "Best model found at epoch 5, saving model\n",
      "[ Train | 007/050 ] loss = 1.36679, acc = 0.79000\n",
      "[ Valid | 007/050 ] loss = 0.97039, acc = 0.83482\n",
      "[ Valid | 007/050 ] loss = 0.97039, acc = 0.83482\n",
      "[ Train | 008/050 ] loss = 1.37321, acc = 0.76647\n",
      "[ Valid | 008/050 ] loss = 0.96919, acc = 0.83482\n",
      "[ Valid | 008/050 ] loss = 0.96919, acc = 0.83482\n",
      "[ Train | 009/050 ] loss = 1.40335, acc = 0.76986\n",
      "[ Valid | 009/050 ] loss = 0.96966, acc = 0.83817\n",
      "[ Valid | 009/050 ] loss = 0.96966, acc = 0.83817\n",
      "[ Train | 010/050 ] loss = 1.37785, acc = 0.76657\n",
      "[ Valid | 010/050 ] loss = 0.96656, acc = 0.83668\n",
      "[ Valid | 010/050 ] loss = 0.96656, acc = 0.83668\n",
      "[ Train | 011/050 ] loss = 1.38072, acc = 0.77504\n",
      "[ Valid | 011/050 ] loss = 0.93323, acc = 0.84003\n",
      "[ Valid | 011/050 ] loss = 0.93323, acc = 0.84003\n",
      "[ Train | 012/050 ] loss = 1.41735, acc = 0.75828\n",
      "[ Valid | 012/050 ] loss = 0.94750, acc = 0.83780\n",
      "[ Valid | 012/050 ] loss = 0.94750, acc = 0.83780\n",
      "[ Train | 013/050 ] loss = 1.39063, acc = 0.76901\n",
      "[ Valid | 013/050 ] loss = 0.95043, acc = 0.83891\n",
      "[ Valid | 013/050 ] loss = 0.95043, acc = 0.83891\n",
      "[ Train | 014/050 ] loss = 1.40046, acc = 0.76186\n",
      "[ Valid | 014/050 ] loss = 0.97646, acc = 0.83408\n",
      "[ Valid | 014/050 ] loss = 0.97646, acc = 0.83408\n",
      "[ Train | 015/050 ] loss = 1.38628, acc = 0.77014\n",
      "[ Valid | 015/050 ] loss = 0.98140, acc = 0.83594\n",
      "[ Valid | 015/050 ] loss = 0.98140, acc = 0.83594\n",
      "[ Train | 016/050 ] loss = 1.40897, acc = 0.76431\n",
      "[ Valid | 016/050 ] loss = 0.96254, acc = 0.83519\n",
      "[ Valid | 016/050 ] loss = 0.96254, acc = 0.83519\n",
      "[ Train | 017/050 ] loss = 1.37663, acc = 0.77278\n",
      "[ Valid | 017/050 ] loss = 0.93468, acc = 0.84152\n",
      "[ Valid | 017/050 ] loss = 0.93468, acc = 0.84152 -> best\n",
      "Best model found at epoch 16, saving model\n",
      "[ Train | 018/050 ] loss = 1.37488, acc = 0.78069\n",
      "[ Valid | 018/050 ] loss = 0.95108, acc = 0.83891\n",
      "[ Valid | 018/050 ] loss = 0.95108, acc = 0.83891\n",
      "[ Train | 019/050 ] loss = 1.37993, acc = 0.77504\n",
      "[ Valid | 019/050 ] loss = 0.97722, acc = 0.83780\n",
      "[ Valid | 019/050 ] loss = 0.97722, acc = 0.83780\n",
      "[ Train | 020/050 ] loss = 1.40029, acc = 0.76073\n",
      "[ Valid | 020/050 ] loss = 0.95298, acc = 0.83594\n",
      "[ Valid | 020/050 ] loss = 0.95298, acc = 0.83594\n",
      "[ Train | 021/050 ] loss = 1.37482, acc = 0.77748\n",
      "[ Valid | 021/050 ] loss = 0.97792, acc = 0.83147\n",
      "[ Valid | 021/050 ] loss = 0.97792, acc = 0.83147\n",
      "[ Train | 022/050 ] loss = 1.37915, acc = 0.77146\n",
      "[ Valid | 022/050 ] loss = 0.93305, acc = 0.84338\n",
      "[ Valid | 022/050 ] loss = 0.93305, acc = 0.84338 -> best\n",
      "Best model found at epoch 21, saving model\n",
      "[ Train | 023/050 ] loss = 1.40160, acc = 0.76638\n",
      "[ Valid | 023/050 ] loss = 0.97667, acc = 0.83557\n",
      "[ Valid | 023/050 ] loss = 0.97667, acc = 0.83557\n",
      "[ Train | 024/050 ] loss = 1.38294, acc = 0.78294\n",
      "[ Valid | 024/050 ] loss = 0.94088, acc = 0.84077\n",
      "[ Valid | 024/050 ] loss = 0.94088, acc = 0.84077\n",
      "[ Train | 025/050 ] loss = 1.37268, acc = 0.78774\n",
      "[ Valid | 025/050 ] loss = 0.93833, acc = 0.83891\n",
      "[ Valid | 025/050 ] loss = 0.93833, acc = 0.83891\n",
      "[ Train | 026/050 ] loss = 1.38131, acc = 0.77268\n",
      "[ Valid | 026/050 ] loss = 0.98250, acc = 0.83445\n",
      "[ Valid | 026/050 ] loss = 0.98250, acc = 0.83445\n",
      "[ Train | 027/050 ] loss = 1.38662, acc = 0.78106\n",
      "[ Valid | 027/050 ] loss = 0.91748, acc = 0.84449\n",
      "[ Valid | 027/050 ] loss = 0.91748, acc = 0.84449 -> best\n",
      "Best model found at epoch 26, saving model\n",
      "[ Train | 028/050 ] loss = 1.38884, acc = 0.76318\n",
      "[ Valid | 028/050 ] loss = 0.95953, acc = 0.84040\n",
      "[ Valid | 028/050 ] loss = 0.95953, acc = 0.84040\n",
      "[ Train | 029/050 ] loss = 1.38217, acc = 0.78153\n",
      "[ Valid | 029/050 ] loss = 0.95662, acc = 0.83705\n",
      "[ Valid | 029/050 ] loss = 0.95662, acc = 0.83705\n",
      "[ Train | 030/050 ] loss = 1.35123, acc = 0.78389\n",
      "[ Valid | 030/050 ] loss = 0.93343, acc = 0.83482\n",
      "[ Valid | 030/050 ] loss = 0.93343, acc = 0.83482\n",
      "[ Train | 031/050 ] loss = 1.36558, acc = 0.78944\n",
      "[ Valid | 031/050 ] loss = 0.93324, acc = 0.84263\n",
      "[ Valid | 031/050 ] loss = 0.93324, acc = 0.84263\n",
      "[ Train | 032/050 ] loss = 1.37818, acc = 0.77796\n",
      "[ Valid | 032/050 ] loss = 0.96748, acc = 0.83408\n",
      "[ Valid | 032/050 ] loss = 0.96748, acc = 0.83408\n",
      "[ Train | 033/050 ] loss = 1.40774, acc = 0.76440\n",
      "[ Valid | 033/050 ] loss = 0.95073, acc = 0.83854\n",
      "[ Valid | 033/050 ] loss = 0.95073, acc = 0.83854\n",
      "[ Train | 034/050 ] loss = 1.35634, acc = 0.78483\n",
      "[ Valid | 034/050 ] loss = 0.95599, acc = 0.83668\n",
      "[ Valid | 034/050 ] loss = 0.95599, acc = 0.83668\n",
      "[ Train | 035/050 ] loss = 1.34956, acc = 0.79735\n",
      "[ Valid | 035/050 ] loss = 0.93932, acc = 0.83817\n",
      "[ Valid | 035/050 ] loss = 0.93932, acc = 0.83817\n",
      "[ Train | 036/050 ] loss = 1.35597, acc = 0.78351\n",
      "[ Valid | 036/050 ] loss = 0.95355, acc = 0.83966\n",
      "[ Valid | 036/050 ] loss = 0.95355, acc = 0.83966\n",
      "[ Train | 037/050 ] loss = 1.40888, acc = 0.76685\n",
      "[ Valid | 037/050 ] loss = 0.94390, acc = 0.83929\n",
      "[ Valid | 037/050 ] loss = 0.94390, acc = 0.83929\n",
      "[ Train | 038/050 ] loss = 1.37910, acc = 0.78276\n",
      "[ Valid | 038/050 ] loss = 0.95162, acc = 0.83668\n",
      "[ Valid | 038/050 ] loss = 0.95162, acc = 0.83668\n",
      "[ Train | 039/050 ] loss = 1.38298, acc = 0.77890\n",
      "[ Valid | 039/050 ] loss = 0.96957, acc = 0.83966\n",
      "[ Valid | 039/050 ] loss = 0.96957, acc = 0.83966\n",
      "[ Train | 040/050 ] loss = 1.38223, acc = 0.77607\n",
      "[ Valid | 040/050 ] loss = 0.95287, acc = 0.84040\n",
      "[ Valid | 040/050 ] loss = 0.95287, acc = 0.84040\n",
      "[ Train | 041/050 ] loss = 1.39281, acc = 0.77155\n",
      "[ Valid | 041/050 ] loss = 0.93783, acc = 0.84487\n",
      "[ Valid | 041/050 ] loss = 0.93783, acc = 0.84487 -> best\n",
      "Best model found at epoch 40, saving model\n",
      "[ Train | 042/050 ] loss = 1.34172, acc = 0.77636\n",
      "[ Valid | 042/050 ] loss = 0.93396, acc = 0.84077\n",
      "[ Valid | 042/050 ] loss = 0.93396, acc = 0.84077\n",
      "[ Train | 043/050 ] loss = 1.37298, acc = 0.78134\n",
      "[ Valid | 043/050 ] loss = 0.93617, acc = 0.84263\n",
      "[ Valid | 043/050 ] loss = 0.93617, acc = 0.84263\n",
      "[ Train | 044/050 ] loss = 1.37957, acc = 0.77513\n",
      "[ Valid | 044/050 ] loss = 0.96003, acc = 0.83854\n",
      "[ Valid | 044/050 ] loss = 0.96003, acc = 0.83854\n",
      "[ Train | 045/050 ] loss = 1.36019, acc = 0.78200\n",
      "[ Valid | 045/050 ] loss = 0.94996, acc = 0.84115\n",
      "[ Valid | 045/050 ] loss = 0.94996, acc = 0.84115\n",
      "[ Train | 046/050 ] loss = 1.36684, acc = 0.78746\n",
      "[ Valid | 046/050 ] loss = 0.97415, acc = 0.82738\n",
      "[ Valid | 046/050 ] loss = 0.97415, acc = 0.82738\n",
      "[ Train | 047/050 ] loss = 1.37039, acc = 0.78219\n",
      "[ Valid | 047/050 ] loss = 0.97680, acc = 0.82180\n",
      "[ Valid | 047/050 ] loss = 0.97680, acc = 0.82180\n",
      "[ Train | 048/050 ] loss = 1.36261, acc = 0.78426\n",
      "[ Valid | 048/050 ] loss = 0.95483, acc = 0.83482\n",
      "[ Valid | 048/050 ] loss = 0.95483, acc = 0.83482\n",
      "[ Train | 049/050 ] loss = 1.37090, acc = 0.78454\n",
      "[ Valid | 049/050 ] loss = 0.92577, acc = 0.84375\n",
      "[ Valid | 049/050 ] loss = 0.92577, acc = 0.84375\n",
      "[ Train | 050/050 ] loss = 1.38014, acc = 0.79283\n",
      "[ Valid | 050/050 ] loss = 0.94312, acc = 0.84115\n",
      "[ Valid | 050/050 ] loss = 0.94312, acc = 0.84115\n"
     ]
    }
   ],
   "source": [
    "# \"cuda\" only when GPUs are available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# The number of training epochs and patience.\n",
    "n_epochs = 300\n",
    "patience = 100 # If no improvement in 'patience' epochs, early stop\n",
    "\n",
    "# Initialize a model, and put it on the device specified.\n",
    "model = Resnet34().to(device)\n",
    "model.load_state_dict(torch.load(\"medium_best.ckpt\", weights_only=False))\n",
    "\n",
    "# For the classification task, we use cross-entropy as the measurement of performance.\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=5e-4, weight_decay=1e-5) \n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "\n",
    "# Cosine scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-6)\n",
    "\n",
    "# Initialize trackers, these are not parameters and should not be changed\n",
    "stale = 0\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    # Make sure the model is in train mode before training.\n",
    "    model.train()\n",
    "\n",
    "    # These are used to record information in training.\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "        #imgs = imgs.half()\n",
    "        #print(imgs.shape,labels.shape)\n",
    "\n",
    "        # Forward the data. (Make sure data and model are on the same device.)\n",
    "        logits = model(imgs.to(device))\n",
    "\n",
    "        # Calculate the cross-entropy loss.\n",
    "        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "\n",
    "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the gradients for parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient norms for stable training.\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "        # Update the parameters with computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # update lr\n",
    "        scheduler.step()\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        # acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "        # print(logits.shape, '\\n', labels.shape)        \n",
    "        # assert 1 == 2\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device).argmax(dim=-1)).float().mean()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "        \n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "    # Print the information.\n",
    "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "    model.eval()\n",
    "\n",
    "    # These are used to record information in validation.\n",
    "    valid_loss = []\n",
    "    valid_accs = []\n",
    "\n",
    "    # Iterate the validation set by batches.\n",
    "    for batch in valid_loader:\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "        #imgs = imgs.half()\n",
    "\n",
    "        # We don't need gradient in validation.\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "            logits = model(imgs.to(device))\n",
    "\n",
    "        # We can still compute the loss (but not the gradient).\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        valid_loss.append(loss.item())\n",
    "        valid_accs.append(acc)\n",
    "        #break\n",
    "\n",
    "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "\n",
    "    # Print the information.\n",
    "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "\n",
    "\n",
    "    # update logs\n",
    "    if valid_acc > best_acc:\n",
    "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
    "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
    "    else:\n",
    "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
    "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "\n",
    "\n",
    "    # save models\n",
    "    if valid_acc > best_acc:\n",
    "        print(f\"Best model found at epoch {epoch}, saving model\")\n",
    "        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n",
    "        best_acc = valid_acc\n",
    "        stale = 0\n",
    "    else:\n",
    "        stale += 1\n",
    "        if stale > patience:\n",
    "            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.493644,
     "end_time": "2022-02-23T19:10:19.985992",
     "exception": false,
     "start_time": "2022-02-23T19:10:19.492348",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One ./food11\\test sample ./food11\\test\\0001.jpg\n"
     ]
    }
   ],
   "source": [
    "test_set = FoodDataset(os.path.join(_dataset_dir,\"test\"), tfm=test_tfm, is_test=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.498773,
     "end_time": "2022-02-23T19:10:20.961802",
     "exception": false,
     "start_time": "2022-02-23T19:10:20.463029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Testing and generate prediction CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 49.157727,
     "end_time": "2022-02-23T19:11:10.61523",
     "exception": false,
     "start_time": "2022-02-23T19:10:21.457503",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_best = Resnet34().to(device)\n",
    "model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\", weights_only=False))\n",
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        test_pred = model_best(data.to(device))\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        prediction += test_label.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 0.554276,
     "end_time": "2022-02-23T19:11:11.870035",
     "exception": false,
     "start_time": "2022-02-23T19:11:11.315759",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pre_csv(preds, subname):\n",
    "    #create test csv\n",
    "    def pad4(i):\n",
    "        return \"0\"*(4-len(str(i)))+str(i)\n",
    "    df = pd.DataFrame()\n",
    "    df[\"Id\"] = [pad4(i) for i in range(1,len(test_set)+1)]\n",
    "    df[\"Category\"] = preds\n",
    "    df.to_csv(subname,index = False)\n",
    "pre_csv(prediction, \"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Time Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One ./food11\\test sample ./food11\\test\\0001.jpg\n",
      "One ./food11\\test sample ./food11\\test\\0001.jpg\n",
      "One ./food11\\test sample ./food11\\test\\0001.jpg\n",
      "One ./food11\\test sample ./food11\\test\\0001.jpg\n",
      "One ./food11\\test sample ./food11\\test\\0001.jpg\n",
      "One ./food11\\test sample ./food11\\test\\0001.jpg\n",
      "One ./food11\\test sample ./food11\\test\\0001.jpg\n",
      "preds_np shape: (7, 3347, 11)\n",
      "bb shape: (3347, 11)\n"
     ]
    }
   ],
   "source": [
    "# Test Time Augmentation\n",
    "# 1个使用test_tfm测试集\n",
    "test_set = FoodDataset(os.path.join(_dataset_dir, \"test\"), tfm=test_tfm, is_test=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# 5个使用train_tfm测试集\n",
    "test_loaders = []\n",
    "for i in range(5):\n",
    "    test_set_i = FoodDataset(os.path.join(_dataset_dir, 'test'), tfm=train_tfm, is_test=True)\n",
    "    test_loader_i = DataLoader(test_set_i, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loaders.append(test_loader_i)\n",
    "# model_best = Classifier().to(device)\n",
    "model_best = Resnet34()\n",
    "model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\", weights_only=False))\n",
    "model_best = model_best.to(device)\n",
    "model_best.eval()\n",
    "\n",
    "# preds存放在6个测试集(1+5)上的测试结果矩阵，每个矩阵是(3347,11)\n",
    "preds = [[], [], [], [], [], [], []]\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    # 用test_tfm的测试集\n",
    "    for data in test_loader:\n",
    "        test_preds = model_best(data.to(device)).cpu().data.numpy()\n",
    "        preds[0].extend(test_preds)\n",
    "    # 5个用train_tfm的测试集\n",
    "    for i, loader in enumerate(test_loaders):\n",
    "        for data in loader:\n",
    "            test_preds = model_best(data.to(device)).cpu().data.numpy()\n",
    "            preds[i+1].extend(test_preds)\n",
    "\n",
    "preds_np = np.array(preds, dtype=object)\n",
    "print('preds_np shape: {}'.format(preds_np.shape))\n",
    "# 对6个测试结果加权求和\n",
    "bb = 0.5 * preds_np[0] + 0.1 * preds_np[1] + 0.1 * preds_np[2] + 0.1 * preds_np[3] + 0.1 * preds_np[4] + 0.1 * preds_np[5]\n",
    "print('bb shape: {}'.format(bb.shape))\n",
    "prediction = np.argmax(bb, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_csv(prediction, 'submission_resnet34_ttm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vote for most frequent result, pereds_np shape [6, 3347, 11], so preds_np[0] get 2 votes, others get 1 vote\n",
    "def voting(preds, num_classes=11):\n",
    "    cls = np.argmax(preds, axis=-1)                     # (6, 3347)\n",
    "    weights = np.array([2, 1, 1, 1, 1, 1], dtype=float)\n",
    "    # 对每个样本沿 axis=0（6 次预测）做加权众数\n",
    "    final = np.apply_along_axis(\n",
    "        lambda x: np.bincount(x, weights=weights, minlength=num_classes).argmax(),\n",
    "        axis=0, arr=cls\n",
    "    )                                                   # (3347,)\n",
    "    return final\n",
    "\n",
    "pre_csv(voting(preds_np, 11), 'submission_resnet34_ttm_vote.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 3300998,
     "sourceId": 34954,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30171,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
